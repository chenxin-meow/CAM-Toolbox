{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lF4N3nBtqnG9"
   },
   "source": [
    "# Iterative Methods for Solving Linear Systems (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A general framework of Iterative Methods\n",
    "\n",
    "Goal: Solve a large linear system\n",
    "\n",
    "$$\n",
    "A\\vec{x}=\\vec{f},\n",
    "$$\n",
    "\n",
    "\n",
    "where $A\\in M_{n\\times n}(\\mathbb C)$ is a large matrix, $\\vec f\\in \\mathbb C^n$ is a vecor.\n",
    "\n",
    "The general idea is of Iterative Method is to develop an iterative scheme, that is we want to find a sequenence of vector $\\vec{x}^0,\\vec{x}^1,\\cdots$ such that\n",
    "\n",
    "$$\n",
    "\\vec{x_k}\\to \\vec{x}^*\\qquad as \\qquad k\\to \\infty,\n",
    "$$\n",
    "\n",
    "where $\\vec{x}^*$ is the analytic solution of $A\\vec{x}=\\vec{f}$.\n",
    "\n",
    "To invent a general iterative scheme, we choose a matrix $N$ and **split** matrix $A$ into\n",
    "\n",
    "$$\n",
    "A=N-(N-A)=N-P.\n",
    "$$\n",
    "\n",
    "The choice of $N$ can vary under different situation. There are some classical choices of $N$, and we will discuss them later. Now that we have this splitting equation, we have\n",
    "\n",
    "$$\n",
    "A\\vec{x}=\\vec{f} \\iff (N-P)\\vec{x}=\\vec{f} \\iff N\\vec{x}=P\\vec{x}+\\vec{f}.\n",
    "$$\n",
    "\n",
    "From the above equation, we can easily develop an iterative scheme as follows:\n",
    "\n",
    "$$\n",
    "N\\vec{x}^{k+1}=P\\vec{x}^k+\\vec{f}.\n",
    "$$\n",
    "\n",
    "We always choose an invertible $N$, therefore\n",
    "\n",
    "$$\n",
    "\\vec{x}^{k+1}=N^{-1}P\\vec{x}^k+N^{-1}\\vec{f}.\\tag{1}\n",
    "$$\n",
    "\n",
    "Obviously, if the sequence $\\{\\vec x^k\\}_{k=0}^{\\infty}$ converges, it converges to $\\vec{x}^*$, the analytic solution of $A\\vec{x}=\\vec{f}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRKHANM6VLlf"
   },
   "source": [
    "To ensure the convergence, we study the error sequence $\\{\\vec e^k\\}_{k=0}^{\\infty}$ where\n",
    "\n",
    "$$\n",
    "\\vec e^k=\\vec x^k-\\vec x^*.\n",
    "$$\n",
    "\n",
    "Denote $M=N^{-1}P$. From $(1)$ we know that \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\vec{x}^{k+1}&=M\\vec{x}^k+N^{-1}\\vec{f}\\tag{2}\\\\\n",
    "\\vec{x}^*&=M\\vec{x}^*+N^{-1}\\vec{f}\\tag{3}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Using $(3)-(2)$, we get\n",
    "\n",
    "$$\n",
    "\\vec{e}^{k+1}=M\\vec{e}^k=\\cdots=M^k\\vec{e}^0\\tag{4}.\n",
    "$$\n",
    "\n",
    "Since $\\vec{e}^0$ is an arbitrary number depending on the intial value of the iterative scheme, the convergence of $(4)$ is determined by $M^k$. If $M^k$ converges to $0$ as $k$ goes to infinity, then the error sequence will converge to $0$ as well. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCg_NPkqX9V6"
   },
   "source": [
    "### Convergence of $M^k$\n",
    "\n",
    "**Question**: Will $M^k$ converge to $0$?\n",
    "\n",
    "Firstly, we consider a simple case when $M$ is diagonalizable. That is, $M$ has $n$ linearly independent eigenvector $\\{\\vec u_1,\\vec u_2,\\cdots,\\vec u_n\\}\\in \\mathbb C^n$, with $n$ corresponding eigenvalues $\\lambda_1,\\lambda_2,\\cdots,\\lambda_n$ s.t.\n",
    "\n",
    "$$\n",
    "D=Q^{-1}MQ,\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "D=diag(\\lambda_1,\\lambda_2,\\cdots,\\lambda_n)\\qquad \\text{and}\\qquad\n",
    "Q = \n",
    "\\left[\n",
    "  \\begin{array}{cccc}\n",
    "    \\mid & \\mid & & \\mid\\\\\n",
    "    \\vec u_{1} & \\vec u_{2} & \\ldots & \\vec u_{n} \\\\\n",
    "    \\mid & \\mid & & \\mid \n",
    "  \\end{array}\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "WLOG, we assume $\\mid \\lambda_1\\mid \\geq \\mid \\lambda_2\\mid \\geq \\cdots \\geq \\mid \\lambda_n\\mid$. Subsequently, we have\n",
    "\n",
    "$$\n",
    "M\\vec u_i=\\lambda_i \\vec u_i\\qquad\\forall i=1,\\cdots,n.\n",
    "$$\n",
    "\n",
    "Let the initial error to be $\\vec e^0=\\sum_{i=1}^na_i\\vec u_i$. We can always find such $a_i,\\cdots,a_n$ because $\\{\\vec u_1,\\vec u_2,\\cdots,\\vec u_n\\}$ forms a basis in $\\mathbb C^n$.\n",
    "\n",
    "Then,\n",
    "\n",
    "$$\n",
    "\\vec e^k=M^k\\vec e^0=\\sum_{i=1}^n a_i M^k\\vec u_i=\\sum_{i=1}^n a_i \\lambda_i^k\\vec u_i\\leq \\lambda_1^k(a_1\\vec u_1+\\sum_{i=2}^n a_i (\\frac{\\lambda_i}{\\lambda_1})^k\\vec u_i).\\tag{5}\n",
    "$$\n",
    "\n",
    "Since $\\mid \\lambda_1\\mid$ is the largest eigenvalue in magnitude, $(\\frac{\\lambda_i}{\\lambda_1})^k$ will tend to $0$ when $k$ goes to infinity. Therefore, by $(5)$, $\\mid\\mid\\vec e^k\\mid\\mid$ converges to zero if and only if $\\mid \\lambda_1\\mid <1$.\n",
    "\n",
    "Denote $\\rho(M):=\\mid \\lambda_1\\mid=max\\{\\mid\\lambda_k\\mid: \\lambda_k \\text{ is the eigenvalue of }M\\}$. We call $\\rho(M)$ the **spectral radius** of $M$. The necessary and sufficient condition for $M^k$ to converge is  $\\rho(M)<1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rVeJeF42tZr"
   },
   "source": [
    "### Estimating spectral radius $\\rho(M)$\n",
    "\n",
    "We know that the spectral radius $\\rho(M)$ is so important, but how can we determine $\\rho(M)<1$ or not? One can always compute all the eigenvalues by brute force and pick the largest one. One can also use more sophiscated method (e.g.: Power method, QR method) to find the largest eigenvalue in maginitude -- we will discuss it in the next chapter. Alternatively, we do have other way to determine the range of $\\rho(M)=\\mid\\lambda_1\\mid$. By the following theorem, we cannot identify the exact value of $\\rho(M)$, but we can find an upper bound of it, which will also be helpful to our convergence analysis in some cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13JeE5CBy47X"
   },
   "source": [
    "#### Theorem 1 (Gershgorin Circle Theorem)\n",
    "\n",
    "Given a matrix $A=(a_{ij})_{1\\leq i,j\\leq n}\\in M_{n\\times n}(\\mathbb C)$, we consider an eigenvector $\\vec e=(e_1,\\cdots,e_n)^T$ with eigenvalue $\\lambda$. Let $l$ be the index such that $e_l$ is the largest in magnitude of $\\vec e$, i.e. $\\mid e_l\\mid \\geq \\mid e_j\\mid$ for all $j$. Then\n",
    "\n",
    "$$\n",
    "\\lambda\\in \\overline{B_{a_{ll}}(\\sum_{j=1,j\\neq l}^n \\mid a_{lj}\\mid)}.\n",
    "$$\n",
    "\n",
    "\n",
    "*Proof.* Firstly, $A\\vec e=\\lambda \\vec e$.\n",
    "\n",
    "For each $1\\leq i\\leq n$, the $i$-th entry of $A\\vec e=\\lambda \\vec e$ is\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\sum_{j=1}^n a_{ij}e_j&=\\lambda e_i\\\\\n",
    "\\iff a_{ii}e_i+\\sum_{j=1,j\\neq i}^n a_{ij}e_j&=\\lambda e_i\\\\\n",
    "\\iff \\mid(a_{ii}-\\lambda)e_i\\mid&=\\mid -\\sum_{j=1,j\\neq i}^n a_{ij}e_j\\mid\\\\\n",
    "\\iff \\mid a_{ii}-\\lambda\\mid \\cdot \\mid e_i \\mid &\\leq \\sum_{j=1,j\\neq i}^n \\mid a_{ij}\\mid \\cdot \\mid e_j\\mid \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Let $l$ be the index such that $e_l$ is the largest in magnitude of $\\vec e$, i.e. $\\mid e_l\\mid \\geq \\mid e_j\\mid$ for all $j$, then\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\mid a_{ll}-\\lambda\\mid \\cdot \\mid e_l \\mid \\leq \\sum_{j=1,j\\neq l}^n \\mid a_{lj}\\mid \\cdot \\mid e_j\\mid \\leq \\sum_{j=1,j\\neq l}^n \\mid a_{lj}\\mid \\cdot \\mid e_l\\mid\\\\\n",
    "&⇒ \\mid a_{ll}-\\lambda\\mid \\leq \\sum_{j=1,j\\neq l}^n \\mid a_{lj}\\mid\\\\\n",
    "&⇒ \\lambda\\in \\overline{B_{a_{ll}}(\\sum_{j=1,j\\neq l}^n \\mid a_{lj}\\mid)}\\tag{6}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "From above we know that $\\lambda$ lies in a ball of center $a_{ll}$, radius $\\sum_{j=1,j\\neq l}^n \\mid a_{lj}\\mid$. It seems that we are done in finding the bound of $\\lambda$, but we are not. Note that if we have already known $l$, we can determine the ball by looking into the $l$-th row of $A$, then the bound of $\\lambda$ is found. However, the problem is that we do NOT know $l$. To find $l$, we should\n",
    "\n",
    "* find eigenvector $\\vec e$, and\n",
    "* find the largest component $e_l$ in magnitude,\n",
    "\n",
    "but $\\vec e$ is exactly what we are seeking for. It's a tautology.\n",
    "\n",
    "Fortunately, we can play a trick by releasing the requirement in equation $(6)$: we take the **union** of all balls $B_{a_{ii}},\\ i=1,\\cdots,n$, then\n",
    "\n",
    "$$\n",
    "\\lambda\\in  \\bigcup_{i=1}^n \\overline{B_{a_{ii}}(\\sum_{j=1,j\\neq i}^n \\mid a_{ij}\\mid)} \\tag{7}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahFOnWQY8MRI"
   },
   "source": [
    "Theorem 1 leads to an important property of matrix.\n",
    "\n",
    "#### Strictly Dominant Diagonal (SDD) Matrix\n",
    "\n",
    "\n",
    "A matrix $A=(a_{ij})_{1\\leq i,j\\leq n}\\in M_{n\\times n}(\\mathbb C)$ is strictly dominant diagonal (SDD) if\n",
    "\n",
    "$$\n",
    "\\mid a_{ii}\\mid  > \\sum_{j=1,j\\neq i}^n \\mid a_{ij}\\mid ,\\qquad \\forall i=1,\\cdots,n.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Obviously, an SDD matrix $A$ must be non-singular. A quick proof is as follows.\n",
    "\n",
    "By Theorem 1, all eigenvalues of $A$ satisfy\n",
    "\n",
    "$$\n",
    "\\lambda\\in  \\bigcup_{i=1}^n \\overline{B_{a_{ii}}(\\sum_{j=1,j\\neq i}^n \\mid a_{ij}\\mid)} \\subset \\bigcup_{i=1}^n \\overline{B_{a_{ii}}(\\mid a_{ii} \\mid)}.\n",
    "$$\n",
    "\n",
    "Therefore, every ball $\\overline{B_{a_{ii}}(\\sum_{j=1,j\\neq i}^n \\mid a_{ij}\\mid)}$ must NOT touch 0, which implies that NO eigenvalue is 0. So $A$ is non-singular.\n",
    "\n",
    "\n",
    "\n",
    "SDD helps us to study the convergence of iterative method in some situation. We will discuss it later. Now let's introduce some concrete algorithms which utilize iterative methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YANAtV7xusOd"
   },
   "source": [
    "## Different Splitting Methods\n",
    "\n",
    "Look back to our splitting method again:\n",
    "\n",
    "$$\n",
    "A=N-(N-A)=N-P.\n",
    "$$\n",
    "\n",
    "Of course, the splitting method itself is not random, so we need our matrix $N$ has some desirable properties:\n",
    "\n",
    "1. $N$ should be related to $A$, otherwise, it won't reduce the computation cost;\n",
    "2. $N$ should be simple;\n",
    "3. $N$ should have an inverse and $N^{-1}$ is easy to compute;\n",
    "4. The spectral radint of $M=N^{-1}P$ should be small to ensure convergence.\n",
    "\n",
    "There are many possible choices of $N$. Here I will introduce three most popular choices, making up three common iterative methods to solve large linear system: Jacobi mathod, Gauss-Seidal method, and SOR method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSWTnedmwKF7"
   },
   "source": [
    "## Jacobi Method\n",
    "\n",
    "Jacobi Method is the simplest and the most intuitive. We choose \n",
    "\n",
    "$$\n",
    "N=D,\n",
    "$$\n",
    "\n",
    "where matrix $D$ is a diagonal component of $A$.\n",
    "\n",
    "Therefore, we split $A=N-P=D-(D-A)$, and get an iterative scheme as\n",
    "\n",
    "$$\n",
    "D\\vec{x}^{n+1}=(D-A)\\vec{x}^n+\\vec{f},\\qquad\\text{or}\\qquad\\vec{x}^{n+1}=D^{-1}(D-A)\\vec{x}^n+D^{-1}\\vec{f}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Izyh6sis019u"
   },
   "source": [
    "### Code demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Qt5gFqlA19CC"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from numpy import array, zeros, diag, diagflat, dot\n",
    "from numpy import linalg\n",
    "\n",
    "def jacobi(A, b, tol=10**(-7), N=25, x=None):\n",
    "    \"\"\"Solves the equation Ax=b via the Jacobi iterative method.\"\"\"\n",
    "    # Create an initial guess if needed                                                                                                                                                            \n",
    "    if x is None:\n",
    "        x = zeros(len(A[0]))\n",
    "\n",
    "    # Create a vector of the diagonal elements of A                                                                                                                                                \n",
    "    # and subtract them from A                                                                                                                                                                     \n",
    "    D = diag(A)\n",
    "    R = A - diagflat(D)\n",
    "\n",
    "    # Iterate for N times                                                                                                                                                                          \n",
    "    for i in range(N):\n",
    "        x_iter = (b - dot(R,x)) / D\n",
    "        tol_iter = linalg.norm(x_iter-x)\n",
    "        if(tol_iter < tol): \n",
    "          x = x_iter\n",
    "          break\n",
    "        x = x_iter\n",
    "    return x, i, tol_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nyC6I1oo2EeH"
   },
   "source": [
    "Consider the linear system\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "5 & -2 & 3\\\\\n",
    "-3 & 9 & 1\\\\\n",
    "2 & -1 & -7\\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix}\n",
    "-1 \\\\\n",
    "2 \\\\\n",
    "3 \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Jacobi method starts from $\\vec x^0=(0,0,0)^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3pEdjJ_P3zSy",
    "outputId": "08216931-2fdb-4b78-d58b-1df678be4c9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 5, -2,  3],\n",
      "       [-3,  9,  1],\n",
      "       [ 2, -1, -7]])\n",
      "array([-1,  2,  3])\n"
     ]
    }
   ],
   "source": [
    "A = array([[5,-2,3],[-3,9,1],[2,-1,-7]])\n",
    "pprint(A)\n",
    "b = array([-1,2,3])\n",
    "pprint(b)\n",
    "guess = array([0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OwVX9I7X4LPm",
    "outputId": "a0f8d3d8-6c64-4ea7-f125-b48d4643c66b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.18611987,  0.33123028, -0.42271294]), 13, 5.8627929119575755e-08)\n"
     ]
    }
   ],
   "source": [
    "sol_jacobi = jacobi(A,b,N=25,x=guess)\n",
    "pprint(sol_jacobi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9XsytYz57mM"
   },
   "source": [
    "From the code above, after 13 iterations, Jacobi converges under the tolerence threshold $10^{-7}$, giving result $\\vec x^{13}=(0.186,0.331,-0.423)^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0E2HW9B9J3R"
   },
   "source": [
    "## Convergence of Jacobi\n",
    "\n",
    "\n",
    "\n",
    "To analyse the convergence of Jacobi method, we should study the spectral radius of \n",
    "\n",
    "$$\n",
    "M=N^{-1}P=D^{-1}(D-A).\n",
    "$$\n",
    "\n",
    "For the above example,\n",
    "\n",
    "$$\n",
    "A=\\begin{pmatrix}\n",
    "5 & -2 & 3\\\\\n",
    "-3 & 9 & 1\\\\\n",
    "2 & -1 & -7\\\\\n",
    "\\end{pmatrix}\n",
    "=\\begin{pmatrix}\n",
    "5 & 0 & 0\\\\\n",
    "0 & 9 & 0\\\\\n",
    "0 & 0 & -7\\\\\n",
    "\\end{pmatrix}+\n",
    "\\begin{pmatrix}\n",
    "0 & 2 & -3\\\\\n",
    "3 & 0 & -1\\\\\n",
    "-2 & 1 & 0\\\\\n",
    "\\end{pmatrix}\n",
    "=N-P,\n",
    "$$\n",
    "\n",
    "then we have \n",
    "$$\n",
    "M=N^{-1}P=\n",
    "\\begin{pmatrix}\n",
    "0 & 2/5 & -3/5\\\\\n",
    "1/3 & 0 & -1/9\\\\\n",
    "2/7 & -1/7 & 0\\\\\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "\n",
    "where $\\rho(M)=0.267<1$, so Jacobi must converge.\n",
    "\n",
    "Instead of studying $\\rho(M)$, we could derive the convergence directly by special matrix property of $A$ (Theorem 2).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1liJAXUTRtJ"
   },
   "source": [
    "\n",
    "\n",
    "### Theorem 2\n",
    "\n",
    "> If A is SDD, then Jacobi method to solve $A\\vec x=\\vec f$ converges.\n",
    "\n",
    "*Proof*. Let $A=(a_{ij})_{1\\leq i,j\\leq n}$.\n",
    "\n",
    "Jacobi method: \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "D\\vec{x}^{n+1}&=(D-A)\\vec{x}^n+\\vec{f}\\\\\n",
    "⇒ a_{ii}x_i^{n+1}&=-\\sum_{j=1,j\\neq i}^n a_{ij} x_j^n + f_i,\\qquad i=1,\\cdots,n\\\\\n",
    "⇒ x_i^{n+1}&=-\\frac{1}{a_{ii}}\\sum_{j=1,j\\neq i}^n a_{ij} x_j^n + \\frac{1}{a_{ii}} f_i,\\qquad i=1,\\cdots,n.\\tag{8}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Let $x^*$ be the solution of $A\\vec x=\\vec f$ (this $x^*$ always exists because A is SDD hence nonsingular), then\n",
    "\n",
    "\n",
    "$$\n",
    "x_i^{*}=-\\frac{1}{a_{ii}}\\sum_{j=1,j\\neq i}^n a_{ij} x_j^* + \\frac{1}{a_{ii}} f_i,\\qquad i=1,\\cdots,n.\\tag{9}\n",
    "$$\n",
    "\n",
    "By $(8)-(9)$, we have\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "e_i^{n+1}&=-\\frac{1}{a_{ii}}\\sum_{j=1,j\\neq i}^n a_{ij} e_j^n\\\\\n",
    "⇒\\mid e_i^{n+1} \\mid&<\\sum_{j=1,j\\neq i}^n \\frac{\\mid a_{ij}\\mid}{\\mid a_{ii}\\mid} \\mid e_j^n\\mid \\leq \\sum_{j=1,j\\neq i}^n \\frac{\\mid a_{ij}\\mid}{\\mid a_{ii}\\mid} \\|\\vec e^n\\|_{∞},\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $ \\|\\vec e^n\\|_{∞}=\\max_j \\{\\mid e_j^n\\mid\\}$.\n",
    "\n",
    "Note that because $A$ is SDD, \n",
    "\n",
    "$$\n",
    "r=\\max_i\\{\\sum_{j=1,j\\neq i}^n \\frac{\\mid a_{ij}\\mid}{\\mid a_{ii}\\mid}\\}<1.\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\n",
    "\\mid e_i^{n+1} \\mid < r\\|\\vec e^n\\|_{∞},\\qquad i=1,\\cdots,n.\\tag{9}\n",
    "$$\n",
    "\n",
    "As $(9)$ is true for all $i$, so\n",
    "\n",
    "$$\n",
    "\\|\\vec e^{n+1}\\|_{∞}<r\\|\\vec e^n\\|_{∞}.\\tag{10}\n",
    "$$\n",
    "\n",
    "By $(10)$, and $r<1$, we know that $\\vec e^n\\to 0$ as $n\\to ∞$. Jacobi method converges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGO9MS6kU2nx"
   },
   "source": [
    "## Gauss-Seidal Method\n",
    "\n",
    "Gauss-Seidal Method uses another splitting rule. We choose \n",
    "$$\n",
    "N=L+D,\n",
    "$$\n",
    "where matrix $D$ is a diagonal component of $A$, and $L$ is a lower triangular component of $A$.\n",
    "\n",
    "Remark: any matrix $A$ can be splitted into \n",
    "\n",
    "$$\n",
    "A=L+D+U=\\text{lower triangular}+\\text{diagonal}+\\text{upper triangular}.\n",
    "$$\n",
    "\n",
    "Therefore, we split $A=N-P=(L+D)-(-U)$, and get an iterative scheme as\n",
    "\n",
    "$$\n",
    "(L+D)\\vec{x}^{n+1}=-U\\vec{x}^n+\\vec{f},\\qquad\\text{or}\\qquad\\vec{x}^{n+1}=-(L+D)^{-1}U\\vec{x}^n+(L+D)^{-1}\\vec{f}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkEXLpQZ8Biv"
   },
   "source": [
    "### Code demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZFiwC-21AFKA"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from numpy import array, zeros, diag, diagflat, dot, triu, tril\n",
    "from numpy import linalg\n",
    "\n",
    "def gaussSeidal(A, b, tol=10**(-7), N=25, x=None):\n",
    "    \"\"\"Solves the equation Ax=b via the Gauss-Seidal iterative method.\"\"\"\n",
    "    # Create an initial guess if needed                                                                                                                                                            \n",
    "    if x is None:\n",
    "        x = zeros(len(A[0]))                                                                                                                                                                   \n",
    "\n",
    "    U = triu(A, 1)\n",
    "    R = tril(A) # L+D\n",
    "\n",
    "    # Iterate for N times                                                                                                                                                                          \n",
    "    for i in range(N):\n",
    "        x_iter = linalg.inv(R) @ (b - U@x) \n",
    "        tol_iter = linalg.norm(x_iter-x)\n",
    "        x = x_iter\n",
    "        if(tol_iter < tol): \n",
    "          break\n",
    "    return x, i, tol_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etVnaYNafukK"
   },
   "source": [
    "Consider the linear system (same as the example in Jacobi)\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "5 & -2 & 3\\\\\n",
    "-3 & 9 & 1\\\\\n",
    "2 & -1 & -7\\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix}\n",
    "-1 \\\\\n",
    "2 \\\\\n",
    "3 \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Gauss-Seidal method starts from $\\vec x^0=(0,0,0)^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3KNuQ2Q4Ab90",
    "outputId": "556262e3-0094-4ede-c7d7-8d739284cbe1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 5, -2,  3],\n",
      "       [-3,  9,  1],\n",
      "       [ 2, -1, -7]])\n",
      "array([-1,  2,  3])\n"
     ]
    }
   ],
   "source": [
    "A = array([[5,-2,3],[-3,9,1],[2,-1,-7]])\n",
    "pprint(A)\n",
    "b = array([-1,2,3])\n",
    "pprint(b)\n",
    "guess = array([0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aZeCcVdyjzqv",
    "outputId": "81943c1f-a85b-4803-eea6-6f0f7b897fb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.18611986,  0.33123028, -0.42271294]), 8, 6.47998626554141e-08)\n"
     ]
    }
   ],
   "source": [
    "sol_gaussSeidal = gaussSeidal(A,b,N=25,x=guess)\n",
    "pprint(sol_gaussSeidal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2PH83U3f7Ax"
   },
   "source": [
    "From the code above, after 8 iterations, Gauss-Seidal converges under the tolerence threshold $10^{-7}$, giving result $\\vec x^{13}=(0.186,0.331,-0.423)^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSlCcETckG_1"
   },
   "source": [
    "### Convergence of Gauss-Seidal\n",
    "\n",
    "To analyse the convergence of Gauss-Seidal method, we should study the spectral radius of \n",
    "\n",
    "$$\n",
    "M=N^{-1}P=(L+D)^{-1}(-U).\n",
    "$$\n",
    "\n",
    "\n",
    "For the above example,\n",
    "\n",
    "$$\n",
    "A=\\begin{pmatrix}\n",
    "5 & -2 & 3\\\\\n",
    "-3 & 9 & 1\\\\\n",
    "2 & -1 & -7\\\\\n",
    "\\end{pmatrix}\n",
    "=\\begin{pmatrix}\n",
    "5 & 0 & 0\\\\\n",
    "-3 & 9 & 0\\\\\n",
    "2 & -1 & -7\\\\\n",
    "\\end{pmatrix}+\n",
    "\\begin{pmatrix}\n",
    "0 & 2 & -3\\\\\n",
    "0 & 0 & -1\\\\\n",
    "0 & 0 & 0\\\\\n",
    "\\end{pmatrix}\n",
    "=N-P,\n",
    "$$\n",
    "\n",
    "then we have \n",
    "\n",
    "$$\n",
    "M=N^{-1}P=\n",
    "\\begin{pmatrix}\n",
    "0 & 2/5 & -3/5\\\\\n",
    "0 & 2/15 & -14/45\\\\\n",
    "-1/21 & 1/63 & 1/7\\\\\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "\n",
    "where $\\rho(M)=0.205<1$, so Gauss-Seidal must converge.\n",
    "\n",
    "\n",
    "\n",
    "Similar to Theorem 2 in Jacobi method, instead of studying $\\rho(M)$, we could derive the convergence directly by special matrix property of $A$ (Theorem 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ziRxDK7ntCV0"
   },
   "source": [
    "\n",
    "\n",
    "#### Theorem 3\n",
    "\n",
    "> If A is SDD, then Gauss-Seidal method to solve $A\\vec x=\\vec f$ converges.\n",
    "\n",
    "*Proof*. Let $A=(a_{ij})_{1\\leq i,j\\leq n}$.\n",
    "\n",
    "Gauss-Seidal:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "(L+D)\\vec{x}^{n+1}&=-U\\vec{x}^n+\\vec{f}\\\\\n",
    "⇒ a_{ii}x_i^{n+1}&=-\\sum_{j=1}^{i-1} a_{ij} x_j^{n+1} - \\sum_{j=i+1}^{n} a_{ij} x_j^n + f_i,\\qquad i=1,\\cdots,n \\tag{11}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "Let $x^*$ be the solution of $A\\vec x=\\vec f$ (this $x^*$ always exists because A is SDD hence nonsingular), then\n",
    "\n",
    "\n",
    "$$\n",
    "⇒ a_{ii}x_i^*=-\\sum_{j=1}^{i-1} a_{ij} x_j^* - \\sum_{j=i+1}^{n} a_{ij} x_j^* + f_i,\\qquad i=1,\\cdots,n \\tag{12}\n",
    "$$\n",
    "\n",
    "By $(11)-(12)$, we have\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "a_{ii}e_i^{n+1}&=-\\sum_{j=1}^{i-1} a_{ij} e_j^{n+1} - \\sum_{j=i+1}^{n} a_{ij}e_j^n\\\\\n",
    "e_i^{n+1}&=-\\sum_{j=1}^{i-1} \\frac{a_{ij}}{a_{ii}} e_j^{n+1} - \\sum_{j=i+1}^{n} \\frac{a_{ij}}{a_{ii}}e_j^n\\tag{13}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Now, we want to prove: \n",
    "\n",
    "$$\n",
    "\\|\\vec e^{n+1}\\|_{∞}\\leq r \\|\\vec e^{n}\\|_{∞}\\qquad \\text{for}\\qquad r=\\max_i\\{\\sum_{j=1,j\\neq i}^n \\frac{\\mid a_{ij}\\mid}{\\mid a_{ii}\\mid}\\}<1,\n",
    "$$\n",
    "which is equivalent to prove that\n",
    "\n",
    "$$\n",
    "\\mid e_i^{n+1}\\mid\\leq r \\|\\vec e^{n}\\|_{∞},\\qquad i=1,\\cdots,n.\\tag{14}\n",
    "$$\n",
    "\n",
    "We prove $(14)$ by induction.\n",
    "\n",
    "* For $i=1$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "e_1^{n+1}&= -\\sum_{j=2}^{n} \\frac{a_{1j}}{a_{11}}e_j^n\\\\\n",
    "⇒ \\mid e_1^{n+1}\\mid &\\leq \\sum_{j=2}^{n} \\frac{|a_{1j}|}{|a_{11}|}|e_j^n|\\leq \\|\\vec e^{n}\\|_{∞}\\cdot \\sum_{j=2}^{n} \\frac{|a_{1j}|}{|a_{11}|}\\leq r \\|\\vec e^{n}\\|_{∞}\\qquad \\text{for}\\qquad i=1.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "* Assume $\\mid e_j^{n+1}\\mid\\leq r \\|\\vec e^{n}\\|_{∞}$ for $j=1,\\cdots,i-1$, then\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "|e_i^{n+1}|&=\\sum_{j=1}^{i-1} \\frac{|a_{ij}|}{|a_{ii}|} |e_j^{n+1}| + \\sum_{j=i+1}^{n} \\frac{|a_{ij}|}{|a_{ii}|}|e_j^n|\\\\\n",
    "|e_i^{n+1}|&\\leq r \\|\\vec e^{n}\\|_{∞}\\sum_{j=1}^{i-1} \\frac{|a_{ij}|}{|a_{ii}|}+\\|\\vec e^{n}\\|_{∞} \\sum_{j=i+1}^{n} \\frac{|a_{ij}|}{|a_{ii}|}\\\\\n",
    "&\\leq \\|\\vec e^{n}\\|_{∞}\\{\\sum_{j=1,j\\neq i}^{n} \\frac{|a_{ij}|}{|a_{ii}|}\\}\\\\\n",
    "&\\leq r \\|\\vec e^{n}\\|_{∞}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Proof for $(14)$ is completed. Hence, \n",
    "\n",
    "$$\n",
    "\\|\\vec e^{n+1}\\|_{∞}\\leq r \\|\\vec e^{n}\\|_{∞} \\leq r^{n+1} \\|\\vec e^{0}\\|_{∞}\\to 0 \\qquad \\text{as}\\qquad n\\to ∞.\n",
    "$$\n",
    "\n",
    "So Gauss-Seidal converges.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TM12Cryj9LCX"
   },
   "source": [
    "## Successive over-relaxation Method\n",
    "\n",
    "Recall in G-S method, $A=L+D+U$, so\n",
    "\n",
    "$$\n",
    "L\\vec x^{k+1}+ D\\vec x^{k+1}+ U\\vec x^{k} =\\vec f.\n",
    "$$\n",
    "\n",
    "SOR method makes a samll modification when updating $x^{k+1}$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "&L\\vec x^{k+1}+ D\\vec y^{k+1}+ U\\vec x^{k} =\\vec f\\\\\n",
    "&\\vec x^{k+1} =\\vec x^{k} + \\omega (\\vec y^{k+1}-\\vec x^{k})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Then $\\vec y^{k+1}=\\frac{1}{\\omega}(\\vec x^{k+1}+(\\omega-1)\\vec x^{k})$, and \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&L\\vec x^{k+1}+ \\frac{1}{\\omega}D\\vec (\\vec x^{k+1}+(\\omega-1)\\vec x^{k})+ U\\vec x^{k} =\\vec f\\\\\n",
    "&\\implies (L+\\frac{1}{\\omega}D) \\vec x^{k+1} = [\\frac{1}{\\omega}D-(D+U)] \\vec x^{k} +\\vec f.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Actually, SOR Method is similar to G-S method. We choose \n",
    "\n",
    "$$\n",
    "N=L+\\frac{1}{\\omega}D.\n",
    "$$\n",
    "When $\\omega=1$, SOR degenerates to G-S method.\n",
    "\n",
    "Remark: any matrix $A$ can be splitted into \n",
    "\n",
    "$$\n",
    "A=L+D+U=\\text{lower triangular}+\\text{diagonal}+\\text{upper triangular}.\n",
    "$$\n",
    "\n",
    "Therefore, we split $A=N-P=(L+\\frac{1}{\\omega}D)-(\\frac{1}{\\omega}D-(D+U))$, and get an iterative scheme as\n",
    "\n",
    "$$\n",
    "N\\vec{x}^{n+1}=P\\vec{x}^n+\\vec{f},\n",
    "$$\n",
    "\n",
    "where $N=(L+\\frac{1}{\\omega}D)$, $P=\\frac{1}{\\omega}D-(D+U)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjICeA4d_Fw_"
   },
   "source": [
    "### Code demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "M6Vg5bChhXo7"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from numpy import array, zeros, diag, diagflat, dot, triu, tril\n",
    "from numpy import linalg\n",
    "\n",
    "def SOR(A, b, w, tol=10**(-7), num=25, x=None):\n",
    "    \"\"\"Solves the equation Ax=b via the SOR iterative method.\"\"\"\n",
    "    # Create an initial guess if needed                                                                                                                                                            \n",
    "    if x is None:\n",
    "        x = zeros(len(A[0])) \n",
    "\n",
    "    D = diag(diag(A))\n",
    "    N = tril(A,-1)+D/w # L+D/w\n",
    "    P = D/w-(D+triu(A,1))\n",
    "\n",
    "    # Iterate for num times                                                                                                                                                                          \n",
    "    for i in range(num):\n",
    "        x_iter = linalg.inv(N) @ (b + P@x) \n",
    "        tol_iter = linalg.norm(x_iter-x)\n",
    "        x = x_iter\n",
    "        if(tol_iter < tol): \n",
    "          break\n",
    "    return x, i, tol_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NH9y4TxnO6E"
   },
   "source": [
    "Consider the linear system (same as the example in Jacobi and Gauss-Seidal)\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "5 & -2 & 3\\\\\n",
    "-3 & 9 & 1\\\\\n",
    "2 & -1 & -7\\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix}\n",
    "-1 \\\\\n",
    "2 \\\\\n",
    "3 \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "SOR method starts from $\\vec x^0=(0,0,0)^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ru1m8EUXju9h",
    "outputId": "71f31aa0-51e7-418a-a6ca-c4aec455aacb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 5, -2,  3],\n",
      "       [-3,  9,  1],\n",
      "       [ 2, -1, -7]])\n",
      "array([-1,  2,  3])\n"
     ]
    }
   ],
   "source": [
    "A = array([[5,-2,3],[-3,9,1],[2,-1,-7]])\n",
    "pprint(A)\n",
    "b = array([-1,2,3])\n",
    "pprint(b)\n",
    "guess = array([0,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYIwjkfgnWa7"
   },
   "source": [
    "If we set $\\omega=1$, then SOR behaves exactly the same as Gauss-Seidal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Lp1m-ZAkUFo",
    "outputId": "8dbb2fe3-c351-4bbe-b520-aadad31d8c7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.18611986,  0.33123028, -0.42271294]), 8, 6.47998626554141e-08)\n"
     ]
    }
   ],
   "source": [
    "sol_SOR = SOR(A,b,w=1,num=25,x=guess)\n",
    "pprint(sol_SOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sq13KW-anel_"
   },
   "source": [
    "If we set $\\omega=1.1$, $\\omega=0.5$, ... then SOR also converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TZyBHqf8nmWH",
    "outputId": "aeb659f2-b94a-48d7-b5ad-a40817dd890c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.18611986,  0.33123028, -0.42271294]), 11, 6.992198514491115e-08)\n"
     ]
    }
   ],
   "source": [
    "sol_SOR = SOR(A,b,w=1.1,num=25,x=guess)\n",
    "pprint(sol_SOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ODKhM1fjoDmJ",
    "outputId": "99ebf77f-df8c-4a4e-cb44-01e732c15fb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.18611929,  0.33122958, -0.42271333]), 24, 8.370950461147372e-07)\n"
     ]
    }
   ],
   "source": [
    "sol_SOR = SOR(A,b,w=0.5,num=25,x=guess)\n",
    "pprint(sol_SOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJIoGa7YoOcF"
   },
   "source": [
    "However, if we choose $\\omega=1.9$, SOR won't converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yKCnZq_toGMr",
    "outputId": "9a7a32f2-f807-4106-f791-dd31150e1ef0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-264.26277509, -454.22627108, -198.89501106]), 24, 987.2950545217178)\n"
     ]
    }
   ],
   "source": [
    "sol_SOR = SOR(A,b,w=1.9,num=25,x=guess)\n",
    "pprint(sol_SOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtrF8peeoW_j"
   },
   "source": [
    "The choice of $\\omega$ will affect the convergence of SOR. We will discuss it now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gh1W5eeBkLsf"
   },
   "source": [
    "### Convergence of SOR\n",
    "\n",
    "To analyse the convergence of SOR method, we should study the spectral radius of \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "M_{\\omega}&=N^{-1}P=(L+\\frac{1}{\\omega}D)^{-1}(\\frac{1}{\\omega}D-(D+U))\\\\\n",
    "&=(\\frac{1}{\\omega}(\\omega L+D))^{-1}(\\frac{1}{\\omega}(D-\\omega(D+U)))\\\\\n",
    "&=(D+\\omega L)^{-1}((1-\\omega)D-\\omega U)\n",
    "\\end{align*}.\n",
    "$$\n",
    "\n",
    "Note that $M_{\\omega}$ depends on $\\omega$. So we can adjust the value of $\\omega$ to make $\\rho(M_{\\omega})$ as small as possible.\n",
    "\n",
    "Firstly, there is a necessary condition for the convergence of SOR method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFY4P5z6_Huc"
   },
   "source": [
    "\n",
    "#### Theorem 4\n",
    "\n",
    "> If SOR method to solve $A\\vec x=\\vec f$ converges, then $0<\\omega<2$. (The converse is not true.)\n",
    "\n",
    "*Proof*. Suppose SOR converges. Then $\\rho(M_{\\omega})=\\max_i\\{|\\lambda_i|:\\lambda_i\\text{ is eigenvalue of }M_{\\omega}\\}<1$.\n",
    "\n",
    "So $|det(M_{\\omega})|=Π_{i=1}^n |\\lambda_i|<[\\rho(M_{\\omega})]^n<1$. Now we look at $det(M_{\\omega})$.\n",
    "\n",
    "By direct calculation,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "det(M_{\\omega})&=det[(D+\\omega L)^{-1}((1-\\omega)D-\\omega U))]\\\\\n",
    "&=[det(D+\\omega L)]^{-1}\\cdot det[(1-\\omega)D-\\omega U]\\\\\n",
    "&=det(D^{-1})\\cdot det[(1-\\omega)D]\\\\\n",
    "&=det[(1-\\omega)^nI_n]\\\\\n",
    "&=(1-\\omega)^n.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Then $\\rho(M_{\\omega})<1⇒|det(M_{\\omega})|<1 \\iff |(1-\\omega)|^n<1 \\iff 0<\\omega<2$.  Proof completed.\n",
    "\n",
    "Note that $|det(M_{\\omega})|<1\\not⇒\\rho(M_{\\omega})<1$, so the converse is not true.\n",
    "\n",
    "* Remark: To find the optimal $\\omega_{opt}$ with smallest $\\rho(M_{\\omega})$, we solve the optimization problem\n",
    "\n",
    "$$\n",
    "\\min_{\\omega} \\lambda_i \\qquad\\text{s.t.} \\qquad 0<\\omega<2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHB3rU7lFmTq"
   },
   "source": [
    "Now we look at the sufficient condition for the convergence of SOR method.\n",
    "\n",
    "#### Theorem 5\n",
    "\n",
    "> If $A$ is SDD, and $0<\\omega\\leq1$, then SOR method to solve $A\\vec x=\\vec f$ converges.\n",
    "\n",
    "*Proof.* Suppose  $A$ is SDD and $0<\\omega\\leq 1$. Need to show: $\\rho(M_{\\omega})<1$.\n",
    "\n",
    "Proof by contradiction, we assume that $\\exists \\lambda$ such that $|\\lambda|\\geq 1$, where $\\lambda$ is eigenvalue of $M_{\\omega}$. \n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&det(\\lambda I-M_{\\omega})=0\\\\\n",
    "&det[\\lambda I-(D+\\omega L)^{-1}((1-\\omega)D-\\omega U)]=0\\\\\n",
    "&det\\{\\lambda(D+\\omega L)^{-1} [(D+\\omega L)-\\frac{1}{\\lambda}((1-\\omega)D-\\omega U)]\\}=0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Note that $\\lambda\\neq 0$, $det(D+\\omega L)\\neq 0$, so\n",
    "\n",
    "$$\n",
    "det(C)=det[(D+\\omega L)-\\frac{1}{\\lambda}((1-\\omega)D-\\omega U)]=0.\n",
    "$$\n",
    "\n",
    "Since $\\omega(1-\\frac{1}{|\\lambda|})\\leq 1-\\frac{1}{|\\lambda|}$, we have \n",
    "\n",
    "$$\n",
    "1-\\frac{1}{|\\lambda|}(1-\\omega)\\geq \\omega \\tag{15}\n",
    "$$\n",
    "\n",
    "We write $C=(c_{ij})$ and $A=(a_{ij})$, then for each $1\\leq i\\leq n$, the diagonal part\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "|c_{ii}|&=|1-\\frac{1}{\\lambda}(1-\\omega)a_{ii}|\\\\\n",
    "&\\geq [1-\\frac{1}{|\\lambda|}(1-\\omega)] |a_{ii}|\\\\\n",
    "&\\geq \\omega |a_{ii}| \\tag{by (15)}\\\\\n",
    "&>\\omega\\sum_{j=1,j\\neq i}^n|a_{ij}| \\tag{A is SDD}\\\\\n",
    "&=\\omega\\sum_{j=1}^{i-1}|a_{ij}| + \\omega\\sum_{j=i+1}^{n}|a_{ij}|\\\\\n",
    "&\\geq \\omega\\sum_{j=1}^{i-1}|a_{ij}| + \\frac{\\omega}{|\\lambda|}\\sum_{j=i+1}^{n}|a_{ij}|\\\\\n",
    "&=\\sum_{j=1,j\\neq i}^n|c_{ij}|.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The last equation holds because the non-diagonal part of $C$ is $\\omega L+\\frac{\\omega}{\\lambda}U$.\n",
    "\n",
    "Therefore, $C$ is SDD, so $C$ is non-singular, $det(C)\\neq 0$, contradiction arises.\n",
    "\n",
    "Hence $\\forall |\\lambda|<1$, that is $\\rho(M_{\\omega})<1$. Proof completed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jOPPvZZCkW_-"
   },
   "source": [
    "### Optimal parameter selection\n",
    "\n",
    "In SOR method, the spectral radius $\\rho(M_{\\omega})$ depends on $\\omega$, now we introduce a theorem to find the optimal $\\omega$ giving a smallest $\\rho(M_{\\omega})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "poapBuEhk--z"
   },
   "source": [
    "#### Consistently ordered Matrix\n",
    "> Let $A=L+D+U$. If the eigenvalues of $\\alpha D^{-1}L+\\frac{1}{\\alpha}D^{-1}U\\ (\\alpha\\neq 0)$ are independent of $\\alpha$, then matrix $A$ is said to be consistently ordered. \n",
    "\n",
    "Examples of consistently ordered matrix:\n",
    "\n",
    "$$\n",
    "A=\n",
    "\\begin{pmatrix}\n",
    "10 & 1\\\\\n",
    "1 & 10\\\\\n",
    "\\end{pmatrix}\\implies \\alpha D^{-1}L+\\frac{1}{\\alpha}D^{-1}U=\n",
    "\\begin{pmatrix}\n",
    "0 & \\frac{1}{10\\alpha}\\\\\n",
    "-\\frac{\\alpha}{10} & 0\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Char. poly: $\\lambda^2-\\frac{1}{100}=0\\implies \\lambda=\\pm \\frac{1}{10}$ are independent of $\\alpha$.\n",
    "\n",
    "* Tridiagonal matrix \n",
    "  \n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\lambda_1 & * &  &  & \\\\\n",
    "* & \\lambda_2 & * &  & \\\\\n",
    "& * & \\lambda_3 & \\ddots &\\\\\n",
    " &  & * & \\ddots & *\\\\\n",
    " &  & & * & \\lambda_n\\\\\n",
    "\\end{pmatrix} \n",
    "$$\n",
    "\n",
    "is consistently ordered. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quq3Lc6On3Zk"
   },
   "source": [
    "#### Theorem 6 (D. Young)\n",
    "Consider a linear system $A\\vec x=\\vec f$.\n",
    "\n",
    "Assume that\n",
    "1. $0<\\omega<2$;\n",
    "2. $M_J=N_J^{-1}P_J$ for Jacobi method has only real eigenvalues;\n",
    "3. $\\beta=\\rho(M_J)<1$;\n",
    "4. $A$ is consistently ordered.\n",
    "\n",
    "Then:\n",
    "1. $\\rho(M_{SOR})<1$ (SOR converges).\n",
    "2. Optimal parameter $\\omega_{opt}$ for fastest convergence is $\\omega_{opt}=\\frac{2}{1+\\sqrt{1-\\beta^2}}$, and $\\rho(M_{SOR},\\omega_{opt})=\\omega_{opt}-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3Pu4YlrpRNB"
   },
   "source": [
    "Example.\n",
    "\n",
    "Consider \n",
    "\n",
    "$$\n",
    "A\\vec x=\n",
    "\\begin{pmatrix}\n",
    "10 & 1\\\\\n",
    "1 & 10\\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix}\n",
    "12 \\\\\n",
    "21 \\\\\n",
    "\\end{pmatrix}\n",
    "=\\vec f\n",
    "$$\n",
    "\n",
    "We know that\n",
    "*  $M_J=N_J^{-1}P_J$ for Jacobi method has only real eigenvalues $\\lambda=\\pm \\frac{1}{10}$;\n",
    "* $\\beta=\\rho(M_J)=\\frac{1}{10}<1$;\n",
    "* $A$ is consistently ordered.\n",
    "\n",
    "By Theorem 6, \n",
    "\n",
    "* $\\rho(M_{SOR})<1$ (SOR converges);\n",
    "* Optimal parameter $\\omega_{opt} =\\frac{2}{1+\\sqrt{1-\\beta^2}}=\\frac{2}{1+\\sqrt{1-\\frac{1}{100}}}=1.0025125$;\n",
    "* $\\rho(M_{SOR},\\omega_{opt})=\\omega_{opt}-1=0.0025125$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxqMT_uUuEtt"
   },
   "source": [
    "## Convergence of Iterative Methods in general\n",
    "\n",
    "Here we study the convergence of Iterative Method again from a more general perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjTormNJuVRk"
   },
   "source": [
    "### Theorem 7 (Householder-John)\n",
    "\n",
    "> Suppose $A$ and $(N^*+N-A)$ are self-adjoint and positive definite matrices.\n",
    "Then the iterative scheme $N\\vec x^{(k+1)}=P\\vec x^{(k)}+\\vec b$ converges.\n",
    "\n",
    "Remark (definitions review):\n",
    "\n",
    "* $B$ is self-adjoint if $B^*:=\\overline {B}^T=B$.\n",
    "* $B$ is positive definite if $\\vec x^*B\\vec x>0$ for $\\forall \\vec x\\neq \\vec 0, \\vec x\\in \\mathbb{C}^n$.\n",
    "\n",
    "\n",
    "*Proof*.\n",
    "\n",
    "Suppose $A$ and $(N^*+N-A)$ are self-adjoint and positive definite.\n",
    "\n",
    "Condiser $M=N^{-1}P=N^{-1}(N-A)=I-N^{-1}A$. Need to show: all eigenvalues $\\lambda$ of $M$ satisfy $|\\lambda|<1$.\n",
    "\n",
    "Let $\\lambda$ be an eigenvalues of $M$. Then $M\\vec x=\\lambda \\vec x,\\ \\vec x\\neq \\vec 0$, i.e.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "(I-N^{-1}A)\\vec x&=\\lambda \\vec x\\\\\n",
    "(N-A)\\vec x&=\\lambda N\\vec x\\\\\n",
    "(1-\\lambda)N\\vec x&=A\\vec x\\tag {16}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Note that $\\lambda\\neq 0$. (Otherwise, $A\\vec x=0$, then $\\vec x^*A\\vec x=0$, contradicting to the fact that A is positive definite.)\n",
    "\n",
    "From $(16)$, multiply $\\vec x^*$ on both sides:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "(1-\\lambda)\\vec x^*N\\vec x&=\\vec x^*A\\vec x \\tag{17}\\\\\n",
    "\\vec x^*N\\vec x&=\\frac{1}{1-\\lambda}\\vec x^*A\\vec x \\tag{18}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Take conjugate transpose on both sides of $(17)$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "[(1-\\lambda)\\vec x^*N\\vec x]^*&=[\\vec x^*A\\vec x]^* \\\\\n",
    "(1-\\overline{\\lambda})\\vec x^* N^* \\vec x^{**}&= \\vec x^*A^*\\vec x^{**}\\\\\n",
    "(1-\\overline{\\lambda})\\vec x^* N^* \\vec x&= \\vec x^*A\\vec x\\\\\n",
    "\\vec x^* N^* \\vec x&=\\frac{1}{1-\\overline{\\lambda}}\\vec x^*A\\vec x \\tag{19}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Take $(18)+(19)-\\vec x^* A \\vec x$:\n",
    "\n",
    "$$\n",
    "\\vec x^* (N^*+N-A) \\vec x = \\{\\frac{1}{1-\\lambda}+\\frac{1}{1-\\overline{\\lambda}}-1\\} \\vec x^*A\\vec x\n",
    "$$\n",
    "\n",
    "Since $\\vec x \\neq \\vec 0$ is eigenvector, and both $(N^*+N-A)$, $A$ are positive definite, so $\\vec x^* (N^*+N-A) \\vec x>0, \\vec x^*A\\vec x>0$, which implies\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\frac{1}{1-\\lambda}+\\frac{1}{1-\\overline{\\lambda}}-1=\\frac{1-|\\lambda|^2}{|1-\\lambda|^2}>0\\\\\n",
    "&\\implies 1-|\\lambda|^2 > 0\\\\\n",
    "&\\implies |\\lambda|<1.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So the iterative scheme converges. Proof completed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYaFqEU0BKJv"
   },
   "source": [
    "### Examples for Householder-John"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYWtG5qgBcdO"
   },
   "source": [
    "#### Example 1\n",
    "\n",
    "Consider a real, symmetric, positive definite (tri-diagonal) matrix\n",
    "\n",
    "$$\n",
    "A=\\begin{pmatrix}\n",
    "\\alpha_1 & \\beta_1 &  &  & \\\\\n",
    "\\beta_1 & \\alpha_2 & \\beta_2 &  & \\\\\n",
    "& \\beta_2 & \\alpha_3 & \\ddots &\\\\\n",
    " &  & \\ddots & \\ddots & \\beta_{n-1}\\\\\n",
    " &  & & \\beta_{n-1} & \\alpha_n\\\\\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Prove that Gauss-Seidal to solve $A\\vec x=\\vec b$ converges.\n",
    "\n",
    "*Proof*. First, $A$ is self-adjoint and positive definite.\n",
    "\n",
    "For Gauss-Seidal, \n",
    "\n",
    "$$\n",
    "N=L+D=\\begin{pmatrix}\n",
    "\\alpha_1 & 0 &  &  & \\\\\n",
    "\\beta_1 & \\alpha_2 & 0 &  & \\\\\n",
    "& \\beta_2 & \\alpha_3 & \\ddots &\\\\\n",
    " &  & \\ddots & \\ddots & 0\\\\\n",
    " &  & & \\beta_{n-1} & \\alpha_n\\\\\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "\n",
    "so\n",
    "\n",
    "$$\n",
    "N^*+N-A=\\begin{pmatrix}\n",
    "\\alpha_1 & 0 &  &  & \\\\\n",
    "0 & \\alpha_2 & 0 &  & \\\\\n",
    "& 0 & \\alpha_3 & \\ddots &\\\\\n",
    " &  & \\ddots & \\ddots & 0\\\\\n",
    " &  & & 0 & \\alpha_n\\\\\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Obviously, $N^*+N-A$ is self-adjoint.\n",
    "\n",
    "Now we need to prove that $N^*+N-A$ is positive definite.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&N^*+N-A \\text{ is positive definite}\\\\\n",
    "&\\iff\\alpha_1 x_1^2+\\cdots+\\alpha_n x_n^2>0,\\qquad\\forall \\vec x=(x_1,\\cdots,x_n)^T\\\\\n",
    "&\\iff \\alpha_1,\\cdots,\\alpha_n >0 \\tag{20}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Let $\\vec e_i=(0,0,\\cdots,1,\\cdots,0)^T$ where the $i$-th entry is 1, $1\\leq i\\leq n$.\n",
    "\n",
    "Since $A$ is positive definite, $\\vec e_i^*A\\vec e_i=\\alpha_i>0$ for all $1\\leq i\\leq n$, so $(20)$ is true. Therefore, $N^*+N-A$ is positive definite.\n",
    "\n",
    "By Householder-John Theorem, Gauss-Seidal converges.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3aQHijKFFDh"
   },
   "source": [
    "#### Example 2\n",
    "\n",
    "> Suppose $A$ is real symmetric positive-definite (SPD) matrix.\n",
    "Prove that SOR method to solve $A\\vec x=\\vec b$ converges iff $0<\\omega<2$.\n",
    "\n",
    "\n",
    "*Proof*. Firstly, $A$ is self-adjoint and positive-definite. Now we consider (N^*+N-A).\n",
    "\n",
    "In SOR method,\n",
    "\n",
    "$N=\\frac{1}{\\omega}D+L$, where $L=U^*=U^T$ as $A$ is symmetric. Therefore,\n",
    "\n",
    "$$\n",
    "N^*+N-A=[\\frac{1}{\\omega}D+U]+[\\frac{1}{\\omega}D+L]-A=(\\frac{2}{\\omega}-1)D.\n",
    "$$\n",
    "\n",
    "Easy to observe that\n",
    "\n",
    "* $N^*+N-A$ is self-adjoint.\n",
    "* $N^*+N-A$ is positive-definite iff $(\\frac{2}{\\omega}-1)>0$ iff $0<\\omega<2$.\n",
    "\n",
    "\n",
    "By Householder-John Theorem, SOR converges iff $0<\\omega<2$.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
