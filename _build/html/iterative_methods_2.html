
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Iterative Methods for Solving Linear Systems (2) &#8212; Numerical Analysis</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/exercise.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Eigenvalue Problems" href="eigenvalue.html" />
    <link rel="prev" title="Iterative Methods for Solving Linear Systems (1)" href="iterative_methods_1.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Numerical Analysis</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to the world of Numerical Analysis!
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="iterative_methods_1.html">
   Iterative Methods for Solving Linear Systems (1)
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Iterative Methods for Solving Linear Systems (2)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="eigenvalue.html">
   Eigenvalue Problems
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://chenxin-meow.github.io/Numerical-Analysis/"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://chenxin-meow.github.io/Numerical-Analysis//issues/new?title=Issue%20on%20page%20%2Fiterative_methods_2.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/iterative_methods_2.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#energy-minimization">
   Energy Minimization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-descent-method">
   Gradient Descent Method
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#algorithm-1-gradient-descent-method">
     Algorithm 1 (Gradient Descent Method)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#convergence-analysis-of-gradient-descent-method">
     Convergence analysis of Gradient Descent Method
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#steepest-descent-method">
   Steepest Descent Method
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#algorithm-2-steepest-descent-method">
     Algorithm 2 (Steepest Descent Method)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conjugate-gradient-method">
   Conjugate Gradient Method
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-choice-of-time-step-alpha-k">
     The Choice of Time Step
     <span class="math notranslate nohighlight">
      \(\alpha_k\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-choice-of-search-direction-vec-p-k">
     The Choice of Search Direction
     <span class="math notranslate nohighlight">
      \(\vec p_k\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#algorithm-3-conjugate-gradient-method">
     Algorithm 3 (Conjugate Gradient Method)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#theorem-1-orthogonal-residuals-and-conjugate-directions">
     Theorem 1: Orthogonal Residuals and Conjugate Directions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#theorem-2-cg-method-converges-in-n-steps">
     Theorem 2: CG Method Converges in
     <span class="math notranslate nohighlight">
      \(n\)
     </span>
     steps
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Iterative Methods for Solving Linear Systems (2)</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#energy-minimization">
   Energy Minimization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-descent-method">
   Gradient Descent Method
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#algorithm-1-gradient-descent-method">
     Algorithm 1 (Gradient Descent Method)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#convergence-analysis-of-gradient-descent-method">
     Convergence analysis of Gradient Descent Method
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#steepest-descent-method">
   Steepest Descent Method
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#algorithm-2-steepest-descent-method">
     Algorithm 2 (Steepest Descent Method)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conjugate-gradient-method">
   Conjugate Gradient Method
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-choice-of-time-step-alpha-k">
     The Choice of Time Step
     <span class="math notranslate nohighlight">
      \(\alpha_k\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-choice-of-search-direction-vec-p-k">
     The Choice of Search Direction
     <span class="math notranslate nohighlight">
      \(\vec p_k\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#algorithm-3-conjugate-gradient-method">
     Algorithm 3 (Conjugate Gradient Method)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#theorem-1-orthogonal-residuals-and-conjugate-directions">
     Theorem 1: Orthogonal Residuals and Conjugate Directions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#theorem-2-cg-method-converges-in-n-steps">
     Theorem 2: CG Method Converges in
     <span class="math notranslate nohighlight">
      \(n\)
     </span>
     steps
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="iterative-methods-for-solving-linear-systems-2">
<h1>Iterative Methods for Solving Linear Systems (2)<a class="headerlink" href="#iterative-methods-for-solving-linear-systems-2" title="Permalink to this headline">#</a></h1>
<p>We have introduced 3 iterative methods (Jacobi, Gauss-Seidal, SOR) to solve linear system <span class="math notranslate nohighlight">\(A\vec x=\vec b\)</span> by <strong>splitting</strong> <span class="math notranslate nohighlight">\(A\)</span> into <span class="math notranslate nohighlight">\(A=N-P\)</span>, resulting an iterative scheme</p>
<div class="math notranslate nohighlight">
\[
N\vec x^{(k+1)}=P\vec x^{(k)}+\vec f.
\]</div>
<p>Matrix splitting, as we have discussed, is one approach to construct an iterative scheme. Alternatively, there is another novel approach to get an iterative scheme to solve <span class="math notranslate nohighlight">\(A\vec x=\vec b\)</span>, which requires us to look at the problem from another perspective – energy minimization.</p>
<section id="energy-minimization">
<h2>Energy Minimization<a class="headerlink" href="#energy-minimization" title="Permalink to this headline">#</a></h2>
<p>Consider a linear system <span class="math notranslate nohighlight">\(A\vec x=\vec b\)</span>, where <span class="math notranslate nohighlight">\(A\in M_{n\times n}(\mathbb R)\)</span> is a symmetric positive definite matrix. Then solving <span class="math notranslate nohighlight">\(A\vec x=\vec b\)</span> is equivalent to minimize the “energy” function</p>
<div class="math notranslate nohighlight">
\[
f(\vec x)=\frac{1}{2} \vec x^T A\vec x-\vec b^T\vec x=\frac12 \|A\vec x-\vec b\|^2-\vec b^T\vec b.
\]</div>
<p>The iterative scheme built upon the energy minimization approach looks for a sequence of <span class="math notranslate nohighlight">\(\vec x_{0},\vec x_{1},\cdots,\vec x_{k},\cdots\)</span> to approximate the minimizer of the “energy” function <span class="math notranslate nohighlight">\(f\)</span>, that is:</p>
<div class="math notranslate nohighlight">
\[
\vec x_{k+1}=\vec x_{k}+\alpha_k \vec p_k\qquad k=0,1,2,\cdots
\]</div>
<p>such that <span class="math notranslate nohighlight">\(f(\vec x_{0})&gt;f(\vec x_{1})&gt;\cdots&gt;f(\vec x_{k})&gt;\cdots\)</span> until convergence.</p>
<p>In each iteration, <span class="math notranslate nohighlight">\(\alpha_k \in \mathbb R, \alpha_k&gt;0\)</span> is called the <em>time step</em>, and <span class="math notranslate nohighlight">\(\vec p_k\in \mathbb R^n\)</span> is called the search direction. There are various choices of <span class="math notranslate nohighlight">\(\alpha_k\)</span> and <span class="math notranslate nohighlight">\(\vec p_k\)</span>. In this chapter we will focus on three sets of them, namely Gradient Descent Method, Steepest Descent Method, and Conjugate Gradient Method.</p>
</section>
<section id="gradient-descent-method">
<h2>Gradient Descent Method<a class="headerlink" href="#gradient-descent-method" title="Permalink to this headline">#</a></h2>
<p>Suppose <span class="math notranslate nohighlight">\(A\in M_{n\times n}(\mathbb R)\)</span> is a symmetric positive definite matrix. Then, for the energy function <span class="math notranslate nohighlight">\(f(\vec x)=\frac{1}{2} \vec x^T A\vec x-\vec b^T\vec x\)</span>, the gradient is <span class="math notranslate nohighlight">\(\nabla f(\vec x)=A\vec x-\vec b\)</span>, and the Hessian is <span class="math notranslate nohighlight">\(Hf=A\)</span>.</p>
<p>In gradient descent, we assume a fixed time step <span class="math notranslate nohighlight">\(\alpha&gt;0\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\vec x_{k+1}=\vec x_{k}+\alpha\vec p_k\qquad k=0,1,2,\cdots
\]</div>
<p>so now we are looking into the search direction <span class="math notranslate nohighlight">\(\vec p_k\)</span>.</p>
<p>Taylor expansion at <span class="math notranslate nohighlight">\(\vec x^{(k+1)}\)</span> gives</p>
<div class="math notranslate nohighlight">
\[
f(\vec x_{k+1})=f(\vec x_{k}+\alpha\vec p_k)=f(\vec x_{k})+\alpha \nabla f(\vec x_{k})\vec p_k+\frac{\alpha^2}{2}\vec p_k^THf(\vec x_{k})\vec p_k.
\]</div>
<p>If <span class="math notranslate nohighlight">\(\alpha\)</span> is small enough, then we can choose</p>
<div class="math notranslate nohighlight">
\[
\vec p_k= -\nabla f(\vec x_{k})=-(A\vec x_{k}-\vec b).
\]</div>
<section id="algorithm-1-gradient-descent-method">
<h3>Algorithm 1 (Gradient Descent Method)<a class="headerlink" href="#algorithm-1-gradient-descent-method" title="Permalink to this headline">#</a></h3>
<p>Goal: solving <span class="math notranslate nohighlight">\(A\vec x=\vec b\)</span> for SPD matrix <span class="math notranslate nohighlight">\(A\)</span>.</p>
<ol class="simple">
<li><p>Initilize <span class="math notranslate nohighlight">\(\vec x_0\in \mathbb R^n\)</span>, <span class="math notranslate nohighlight">\(\vec p_0=-(A\vec x_{0}-\vec b)\)</span>.</p></li>
<li><p>For <span class="math notranslate nohighlight">\(k=0,1,2,\cdots\)</span>, repeat until convergence</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\vec x_{k+1}=\vec x_{k}+\alpha\vec p_k\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(\vec p_k=-(A\vec x_{k}-\vec b)\)</span>.</p></li>
</ul>
</li>
</ol>
</section>
<section id="convergence-analysis-of-gradient-descent-method">
<h3>Convergence analysis of Gradient Descent Method<a class="headerlink" href="#convergence-analysis-of-gradient-descent-method" title="Permalink to this headline">#</a></h3>
<p>We need to choose appropriate <span class="math notranslate nohighlight">\(\alpha\)</span> to ensure the convergence of the Gradient Descent Method.</p>
<p>Let <span class="math notranslate nohighlight">\(A\)</span> be an SPD matrix with eigenvalues <span class="math notranslate nohighlight">\(\lambda_1\geq\cdots\geq\lambda_n\)</span>. And let <span class="math notranslate nohighlight">\(\vec x^*\)</span> be the solution of <span class="math notranslate nohighlight">\(A\vec x=\vec b\)</span>, then we have</p>
<div class="math notranslate nohighlight">
\[
\vec x^*=\vec x^*-\alpha(A\vec x^{*}-\vec b)\tag{1}
\]</div>
<p>Also, from the iteation,</p>
<div class="math notranslate nohighlight">
\[
\vec x_{k+1}=\vec x_k-\alpha(A\vec x_{k}-\vec b)\tag{2}
\]</div>
<p>Using <span class="math notranslate nohighlight">\((2)-(1)\)</span>, we get</p>
<div class="math notranslate nohighlight">
\[
\vec e_{k+1}=(I-\alpha A)\vec e_{k}
\]</div>
<p>Obviously, the convergence of the iterative scheme depends on <span class="math notranslate nohighlight">\(\rho (I-\alpha A)\)</span>. To ensure the convergence, we need <span class="math notranslate nohighlight">\(\rho (I-\alpha A)&lt;1\)</span>. Note that the eigenvalues of <span class="math notranslate nohighlight">\((I-\alpha A)\)</span> are <span class="math notranslate nohighlight">\(1-\alpha\lambda_1,1-\alpha\lambda_2,\cdots,1-\alpha\lambda_n\)</span>, so</p>
<div class="math notranslate nohighlight">
\[
\rho (I-\alpha A)&lt;1 \iff |1-\alpha\lambda_j|&lt;1,\forall j\iff \alpha&lt;\frac{2}{\lambda_j},\forall j.
\]</div>
<p>In practice, we often choose <span class="math notranslate nohighlight">\(\alpha=1/\lambda_{max}\)</span>, where <span class="math notranslate nohighlight">\(\lambda_{max}\)</span> can be founded by the Power method in the chapter (Eigenvalue Problem).</p>
<p>Beyond this, the convergence of the Gradient Descent Method is actually depending on the <em>condition number</em> <span class="math notranslate nohighlight">\(\kappa(A)\)</span> of matrix <span class="math notranslate nohighlight">\(A\)</span>, where <span class="math notranslate nohighlight">\(\kappa(A)=\lambda_{max}/\lambda_{min}\)</span>. Note that</p>
<div class="math notranslate nohighlight">
\[
\rho (I-\alpha A)=\max_j |1-\frac{\lambda_j}{\lambda_{max}}|=1-\frac{\lambda_{min}}{\lambda_{max}}=1-\frac{1}{\kappa(A)}.
\]</div>
<p>Therefore, the Gradient Descent Method converges iff</p>
<div class="math notranslate nohighlight">
\[
\rho (I-\alpha A)=1-\frac{1}{\kappa(A)}&lt;1.
\]</div>
<p>The convergence depends on the condition number. If the condition number is BIG, then the convergence is SLOW.</p>
</section>
</section>
<section id="steepest-descent-method">
<h2>Steepest Descent Method<a class="headerlink" href="#steepest-descent-method" title="Permalink to this headline">#</a></h2>
<p>The Gradient Descent Method uses a fixed time step <span class="math notranslate nohighlight">\(\alpha\)</span>, but we can also construct an iterative scheme for <span class="math notranslate nohighlight">\(\alpha_k\)</span> to obtain a faster convergence, leading to the Steepest Descent Method we are discussing here.</p>
<p>Steepest Descent Method updates the search direction in the same way as the Gradient Descent Method</p>
<div class="math notranslate nohighlight">
\[
\vec p_k= -\nabla f(\vec x_{k})=-(A\vec x_{k}-\vec b),
\]</div>
<p>but the whole iterative scheme</p>
<div class="math notranslate nohighlight">
\[
\vec x_{k+1}=\vec x_{k}+\alpha_k \vec p_k\qquad k=0,1,2,\cdots
\]</div>
<p>is different because we update <span class="math notranslate nohighlight">\(\alpha_k\)</span> in each iteration as well.</p>
<p>In <span class="math notranslate nohighlight">\(k\)</span>-th iteration, given <span class="math notranslate nohighlight">\(\vec p_k= -\nabla f(\vec x_{k})=-(A\vec x_{k}-\vec b)\)</span>, we want to find an optimal <span class="math notranslate nohighlight">\(\alpha_k\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
\alpha_k=arg\min_{\alpha&gt;0}f(\vec x_{k+1})=f(\vec x_{k}+\alpha \vec p_k).
\]</div>
<p>That is, we do a <em>line search</em> for <span class="math notranslate nohighlight">\(\alpha\)</span> to find the optimal <span class="math notranslate nohighlight">\(\alpha_k\)</span> that can minimize <span class="math notranslate nohighlight">\(f(\vec x_{k+1})\)</span> in <span class="math notranslate nohighlight">\(k\)</span>-th iteration. In other words, given a search direction <span class="math notranslate nohighlight">\(\vec p_k\)</span>, time step <span class="math notranslate nohighlight">\(\alpha_k\)</span> is the best choice to approximate <span class="math notranslate nohighlight">\(\min f(\vec x)\)</span>. Now we solve for <span class="math notranslate nohighlight">\(\alpha_k\)</span>. At the critical point, the gradient equals to zero, that is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\frac{d}{d\alpha}\Bigr|_{\alpha=\alpha_k}f(\vec x_{k}+\alpha \vec p_k)&amp;=0\\
 \nabla f(\vec x_{k}+\alpha_k \vec p_k)\cdot\vec p_k&amp;=0\\
-[A(\vec x_{k}+\alpha_k \vec p_k)-\vec b]\cdot\vec p_k&amp;=0\\
(A\vec x_{k}-\vec b)\cdot\vec p_k+\alpha_k \vec p_k\cdot A\vec p_k&amp;=0\\
\implies\alpha_k =\frac{-(A\vec x_{k}-\vec b)\cdot\vec p_k}{\vec p_k\cdot A\vec p_k}&amp;=-\frac{\vec p_k\cdot \vec p_k}{\vec p_k\cdot A\vec p_k}
\end{align*}
\end{split}\]</div>
<p>Note that in the Steepest Descent Method, each contiguous steps are orthogonal to each other, i.e.</p>
<div class="math notranslate nohighlight">
\[
\vec p_k\cdot\vec p_{k+1}=0\qquad \forall k=0,1,\cdots
\]</div>
<section id="algorithm-2-steepest-descent-method">
<h3>Algorithm 2 (Steepest Descent Method)<a class="headerlink" href="#algorithm-2-steepest-descent-method" title="Permalink to this headline">#</a></h3>
<p>Goal: solving <span class="math notranslate nohighlight">\(A\vec x=\vec b\)</span> for SPD matrix <span class="math notranslate nohighlight">\(A\)</span>.</p>
<ol class="simple">
<li><p>Initilize <span class="math notranslate nohighlight">\(\vec x_0\in \mathbb R^n\)</span>, <span class="math notranslate nohighlight">\(\vec p_0=-(A\vec x_{0}-\vec b),\alpha_0=-\frac{\vec p_0\cdot \vec p_0}{\vec p_0\cdot A\vec p_0}\)</span>.</p></li>
<li><p>For <span class="math notranslate nohighlight">\(k=0,1,2,\cdots\)</span>, repeat until convergence</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\vec x_{k+1}=\vec x_{k}+\alpha_k\vec p_k\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(\vec p_k=-(A\vec x_{k}-\vec b)\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha_k=-\frac{\vec p_k\cdot \vec p_k}{\vec p_k\cdot A\vec p_k}\)</span>.</p></li>
</ul>
</li>
</ol>
</section>
</section>
<section id="conjugate-gradient-method">
<h2>Conjugate Gradient Method<a class="headerlink" href="#conjugate-gradient-method" title="Permalink to this headline">#</a></h2>
<p>The “zig-zag” iterative scheme in Steepest Descent Method causes inefficiency. Alternatively, Conjugate Gradient Method can give “direct” convergence to the minimizer. Here is an example to illustrate the motivation behind the Conjugate Gradient Method.</p>
<p>Consider an SPD matrix <span class="math notranslate nohighlight">\(A\in M_{2\times 2}(\mathbb R)\)</span>, let <span class="math notranslate nohighlight">\(\vec x^*\)</span> be the solution of <span class="math notranslate nohighlight">\(A\vec x=\vec b\)</span>. The energy function is <span class="math notranslate nohighlight">\(f(\vec x)=\frac{1}{2} \vec x^T A\vec x-\vec b^T\vec x\)</span>.</p>
<p><img alt="" src="_images/CG.jpg" /></p>
<p>In the 2-dim plane, Steepest Descent Method approximates <span class="math notranslate nohighlight">\(\vec x^*\)</span> via the blue line, which is a really long-winded road. Conjugate Gradient Method, however, behaves greedily as illustrating in the pink line. From the initial position <span class="math notranslate nohighlight">\(\vec x_0\)</span>, given the first step <span class="math notranslate nohighlight">\(\vec p_0\)</span>, we want to go to the solution <span class="math notranslate nohighlight">\(\vec x^*\)</span> directly in 1 step via <span class="math notranslate nohighlight">\(\vec p_1\)</span>. Suppose this is true, then we can write <span class="math notranslate nohighlight">\(\vec p_1=c(\vec x^*-\vec x_1)\)</span>. Note that <span class="math notranslate nohighlight">\(A\vec p_1=c(A\vec x^*-A\vec x_1)=c(\vec b-A\vec x_1)\)</span>, then</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
A\vec p_1\cdot \vec p_0&amp;=c(\vec b-A\vec x_1)\cdot\vec p_0\\
&amp;=-c\nabla f(\vec x_1)\cdot\vec p_0\\
&amp;=-c\nabla f(\vec x_0+\alpha_0\vec p_0)\cdot\vec p_0\\
&amp;=-c\frac{d}{d\alpha}\Bigr|_{\alpha=\alpha_0}f(\vec x_{0}+\alpha \vec p_0)
&amp;=0
\end{align*}
\end{split}\]</div>
<p>because <span class="math notranslate nohighlight">\(\alpha_0\)</span> is chosen to be optimal. The relationship</p>
<div class="math notranslate nohighlight">
\[
A\vec p_1\cdot \vec p_0=0
\]</div>
<p>motivates us to choose the search direction in each step. In this way, our algorithm will converges in only 2 iterations.</p>
<p>Now we are ready to discuss the Conjugate Gradient Method. But before that, let us re-visit some important definitions.</p>
<p><strong>(Definition 1)</strong> We say that the set of directions <span class="math notranslate nohighlight">\(\{\vec p_j\}_{j=0}^{k-1}\)</span> is a <em>conjugate set</em> of directions with respect to <span class="math notranslate nohighlight">\(A\)</span> if</p>
<div class="math notranslate nohighlight">
\[
\vec p_j^T A \vec p_i=0,\ \forall 1\leq i,j\leq k_1,i\neq j.
\]</div>
<p><strong>(Definition 2)</strong> Given an SPD matrix <span class="math notranslate nohighlight">\(A\in M_{n\times n}(\mathbb R)\)</span> and two vectors <span class="math notranslate nohighlight">\(\vec u, \vec v\in \mathbb R^n\)</span>. We define the dot product of <span class="math notranslate nohighlight">\(\vec u, \vec v\)</span> with respect to <span class="math notranslate nohighlight">\(A\)</span> to be</p>
<div class="math notranslate nohighlight">
\[
\langle\ \vec u, \vec v \rangle_{A}=\vec u\cdot A \vec v=\vec u^T A \vec v.
\]</div>
<p>The goal of Conjugate Gradient Method is to find</p>
<ol class="simple">
<li><p>Search direction <span class="math notranslate nohighlight">\(\vec p_j(j=0,1,2,\cdots)\)</span> such that <span class="math notranslate nohighlight">\(\vec p_j^T A \vec p_i=0\)</span> for <span class="math notranslate nohighlight">\(i\neq j\)</span>.</p></li>
<li><p>Time step <span class="math notranslate nohighlight">\(\alpha_k\)</span> to be optimal.</p></li>
</ol>
<section id="the-choice-of-time-step-alpha-k">
<h3>The Choice of Time Step <span class="math notranslate nohighlight">\(\alpha_k\)</span><a class="headerlink" href="#the-choice-of-time-step-alpha-k" title="Permalink to this headline">#</a></h3>
<p>Firstly, we look at the choice of time step <span class="math notranslate nohighlight">\(\alpha_k\)</span>, given <span class="math notranslate nohighlight">\(\vec p_0,\vec p_1,\cdots,\vec p_k\)</span>. This is similar to the discussion in the Steepest Descent.</p>
<p>In <span class="math notranslate nohighlight">\(k\)</span>-th iteration, we want to find an optimal <span class="math notranslate nohighlight">\(\alpha_k\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
\alpha_k=arg\min_{\alpha&gt;0}f(\vec x_{k+1})=f(\vec x_{k}+\alpha \vec p_k).
\]</div>
<p>Therefore,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\frac{d}{d\alpha}\Bigr|_{\alpha=\alpha_k}f(\vec x_{k}+\alpha \vec p_k)&amp;=0\\
 \nabla f(\vec x_{k}+\alpha_k \vec p_k)\cdot\vec p_k&amp;=0\\
-[A(\vec x_{k}+\alpha_k \vec p_k)-\vec b]\cdot\vec p_k&amp;=0\\
(A\vec x_{k}-\vec b)\cdot\vec p_k+\alpha_k \vec p_k\cdot A\vec p_k&amp;=0\\
\implies\alpha_k =\frac{-(A\vec x_{k}-\vec b)\cdot\vec p_k}{\vec p_k\cdot A\vec p_k}&amp;=-\frac{\vec r_k\cdot \vec p_k}{\langle\ \vec p_k, \vec p_k \rangle_{A}},
\end{align*}
\end{split}\]</div>
<p>where we define the residual</p>
<div class="math notranslate nohighlight">
\[
\vec r_k=A\vec x_{k}-\vec b=-\nabla f(\vec x_{k}).
\]</div>
</section>
<section id="the-choice-of-search-direction-vec-p-k">
<h3>The Choice of Search Direction <span class="math notranslate nohighlight">\(\vec p_k\)</span><a class="headerlink" href="#the-choice-of-search-direction-vec-p-k" title="Permalink to this headline">#</a></h3>
<p>Secondly, we look at the choice of the search direction <span class="math notranslate nohighlight">\(\\vec p_k\)</span>, given <span class="math notranslate nohighlight">\(\vec p_0,\vec p_1,\cdots,\vec p_k\)</span>.</p>
<p>Let</p>
<div class="math notranslate nohighlight">
\[
\vec p_k=-\vec r_k-\beta_{k-1}\vec p_{k-1},
\]</div>
<p>where <span class="math notranslate nohighlight">\(\vec r_k=-\nabla f(\vec x_{k})\)</span> is the search direction in the Steepest Descent (negative gradient), and <span class="math notranslate nohighlight">\(\beta_{k-1}\)</span> is the “time step” of  <span class="math notranslate nohighlight">\(\vec p_k\)</span> that ensure <span class="math notranslate nohighlight">\(\vec p_{k-1}^T A \vec p_k=0\)</span>. To find out <span class="math notranslate nohighlight">\(\beta_{k-1}\)</span>, observe that</p>
<div class="math notranslate nohighlight">
\[
A\vec p_k=-A\vec r_k-\beta_{k-1}A\vec p_{k-1},
\]</div>
<p>so</p>
<div class="math notranslate nohighlight">
\[
0=\vec p_{k-1}^T A \vec p_k=-\vec p_{k-1}^TA\vec r_k-\beta_{k-1}\vec p_{k-1}^TA\vec p_{k-1},
\]</div>
<p>which implies</p>
<div class="math notranslate nohighlight">
\[
\beta_{k-1}=\frac{\vec p_{k-1}^TA\vec r_k}{\vec p_{k-1}^TA\vec p_{k-1}}=\frac{\langle\ \vec r_k,\vec p_{k-1} \rangle_{A}}{\langle\ \vec p_{k-1},\vec p_{k-1} \rangle_{A}}.
\]</div>
<p>Now we can formulate the whole procedure of the Conjugate Gradient Method.</p>
</section>
<section id="algorithm-3-conjugate-gradient-method">
<h3>Algorithm 3 (Conjugate Gradient Method)<a class="headerlink" href="#algorithm-3-conjugate-gradient-method" title="Permalink to this headline">#</a></h3>
<p>Goal: solving <span class="math notranslate nohighlight">\(A\vec x=\vec b\)</span> for SPD matrix <span class="math notranslate nohighlight">\(A\)</span>.</p>
<ol class="simple">
<li><p>Initilize <span class="math notranslate nohighlight">\(\vec x_0\in \mathbb R^n\)</span>, <span class="math notranslate nohighlight">\(\vec p_0=-\vec r_0=-(A\vec x_{0}-\vec b),\alpha_0=-\frac{\vec p_0\cdot \vec p_0}{\langle\ \vec p_{0},\vec p_{0} \rangle_{A}}\)</span>.</p></li>
<li><p>For <span class="math notranslate nohighlight">\(k=0,1,2,\cdots\)</span>, repeat until convergence</p>
<ul class="simple">
<li><p>Time step <span class="math notranslate nohighlight">\(\alpha_k =-\frac{\vec r_k\cdot \vec p_k}{\langle\ \vec p_k, \vec p_k \rangle_{A}}\)</span> where <span class="math notranslate nohighlight">\(\vec r_{k}=-(A\vec x_{k}-\vec b)\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(\vec x_{k+1}=\vec x_{k}+\alpha_k\vec p_k\)</span>;</p></li>
<li><p>Search direction <span class="math notranslate nohighlight">\(\vec p_{k+1}=-\vec r_{k+1}-\beta_{k}\vec p_{k}\)</span> where <span class="math notranslate nohighlight">\(\vec r_{k+1}=-(A\vec x_{k+1}-\vec b)\)</span> and <span class="math notranslate nohighlight">\(\beta_{k}=\frac{\langle\ \vec r_{k+1},\vec p_{k} \rangle_{A}}{\langle\ \vec p_{k},\vec p_{k} \rangle_{A}}\)</span></p></li>
</ul>
</li>
</ol>
</section>
<section id="theorem-1-orthogonal-residuals-and-conjugate-directions">
<h3>Theorem 1: Orthogonal Residuals and Conjugate Directions<a class="headerlink" href="#theorem-1-orthogonal-residuals-and-conjugate-directions" title="Permalink to this headline">#</a></h3>
<p>With the iterative scheme in the Conjugate Gradient Method, define <span class="math notranslate nohighlight">\(\vec r_{k}=-(A\vec x_{k}-\vec b)\)</span> in <span class="math notranslate nohighlight">\(k\)</span>-th iteration, then</p>
<ul class="simple">
<li><p>(1) <span class="math notranslate nohighlight">\(\vec r_i^T\vec r_j=0\ ,\forall i\neq j\)</span>. (All the residuals are orthogonal to each other.)</p></li>
<li><p>(2) <span class="math notranslate nohighlight">\(\langle\ \vec p_i, \vec p_j \rangle_{A}:=\vec p_i^TA\vec p_j=0\ ,\forall i\neq j.\)</span> (All the search directions are conjugate to each other.)</p></li>
</ul>
<p><em>Proof by induction.</em></p>
<p>Lemma: <span class="math notranslate nohighlight">\(Span\{\vec p_0,\cdots,\vec p_{k-1}\}=Span\{\vec r_0,\cdots,\vec r_{k-1}\}\)</span>.</p>
<p>For <span class="math notranslate nohighlight">\(i,j\leq 1\)</span>,</p>
<p>(1) <span class="math notranslate nohighlight">\(\vec r_0^T\vec r_1=0\)</span> is true because</p>
<div class="math notranslate nohighlight">
\[
0=\frac{d}{d\alpha}\Bigr|_{\alpha=\alpha_0}f(\vec x_{0}+\alpha \vec p_0)= -\nabla f(\vec x_1)\cdot\vec p_0= \vec r_1\cdot-(\vec r_0).
\]</div>
<p>(2) <span class="math notranslate nohighlight">\(\langle\ \vec p_0, \vec p_1\rangle_{A}=0\)</span> is true by definition.</p>
<p>Suppose the statements (1)(2) are true for <span class="math notranslate nohighlight">\(i,j\leq k\)</span>. Want to prove:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\vec r_{k+1}^T\vec r_j=0\)</span> for all <span class="math notranslate nohighlight">\(j=0,1,\cdots,k\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\langle\ \vec p_{k+1}, \vec p_j \rangle_{A}=\vec p_{k+1}^TA\vec p_j=0\)</span> for all <span class="math notranslate nohighlight">\(j=0,1,\cdots,k\)</span>.</p></li>
</ul>
<p>First we look at (1).</p>
<p>Now, for <span class="math notranslate nohighlight">\(j=0,1,\cdots,k-1\)</span>:</p>
<p>Since <span class="math notranslate nohighlight">\(Span\{\vec p_0,\cdots,\vec p_{j}\}=Span\{\vec r_0,\cdots,\vec r_{j}\}\)</span>, so <span class="math notranslate nohighlight">\(\vec r_k^T\vec p_j=0\)</span> (by induction hypothesis, <span class="math notranslate nohighlight">\(\vec r_k^T\vec r_i=0\)</span> for <span class="math notranslate nohighlight">\(i=0,1,\cdots,j\)</span>).</p>
<p>Note that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
&amp;\vec x_{k+1}=\vec x_{k}+\alpha_k\vec p_k\\
\implies&amp; A\vec x_{k+1}-\vec b=A\vec x_{k}-\vec b+\alpha_k A\vec p_k\\ 
\implies&amp; \vec r_{k+1}=\vec r_{k}+\alpha_k A\vec p_k
\end{align*}
\end{split}\]</div>
<p>we have</p>
<div class="math notranslate nohighlight">
\[
\vec r_{k+1}^T\vec p_j=\vec r_{k}^T\vec p_j+\alpha_k \vec p_k^T A\vec p_j=0,\qquad j=0,1,\cdots,k-1
\]</div>
<p>Also, <span class="math notranslate nohighlight">\(\vec r_{k+1}^T\vec p_k=0\)</span> because</p>
<div class="math notranslate nohighlight">
\[
0=\frac{d}{d\alpha}\Bigr|_{\alpha=\alpha_k}f(\vec x_{k}+\alpha \vec p_k)= -\nabla f(\vec x_{k+1})\cdot\vec p_k= \vec r_{k+1}\cdot \vec p_k.
\]</div>
<p>All together, we have</p>
<div class="math notranslate nohighlight">
\[
\vec r_{k+1}^T\vec p_j=0,\qquad j=0,1,\cdots,k.
\]</div>
<p>Since <span class="math notranslate nohighlight">\(Span\{\vec p_0,\cdots,\vec p_{k}\}=Span\{\vec r_0,\cdots,\vec r_{k}\}\)</span>, easy to show</p>
<div class="math notranslate nohighlight">
\[
\vec r_{k+1}^T\vec r_j=0,\qquad j=0,1,\cdots,k.
\]</div>
<p>So (1) is true for the case <span class="math notranslate nohighlight">\((k+1)\)</span>.</p>
<p>Secondly, we want to show that (2) is true for <span class="math notranslate nohighlight">\((k+1)\)</span>.</p>
<p>Consider <span class="math notranslate nohighlight">\(j=0,1,\cdots,k-1\)</span>.</p>
<p>Note that <span class="math notranslate nohighlight">\(\vec r_{j+1}=\vec r_{j}+\alpha_j A\vec p_j\)</span>, so <span class="math notranslate nohighlight">\(A\vec p_j\in Span\{\vec r_{j+1},\vec r_{j}\}\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
\vec r_{k+1}^TA\vec p_j=0,\qquad j=0,1,\cdots,k-1.
\]</div>
<p>For <span class="math notranslate nohighlight">\(\vec p_{k+1}=-\vec r_{k+1}-\beta_{k}\vec p_{k}\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
\vec p_{k+1}^TA\vec p_j=-\vec r_{k+1}A\vec p_j-\beta_{k}\vec p_{k}^TA\vec p_j=0,\qquad j=0,1,\cdots,k-1.
\]</div>
<p>Also, <span class="math notranslate nohighlight">\(\vec p_{k+1}^TA\vec p_k=0\)</span> be definition.</p>
<p>All together, we have</p>
<div class="math notranslate nohighlight">
\[
\vec p_{k+1}^TA\vec p_j=0,\qquad j=0,1,\cdots,k.
\]</div>
<p>So (2) is true for the case <span class="math notranslate nohighlight">\((k+1)\)</span>.</p>
<p>By mathematical induction, (1)(2) is true.</p>
</section>
<section id="theorem-2-cg-method-converges-in-n-steps">
<h3>Theorem 2: CG Method Converges in <span class="math notranslate nohighlight">\(n\)</span> steps<a class="headerlink" href="#theorem-2-cg-method-converges-in-n-steps" title="Permalink to this headline">#</a></h3>
<p>Consider the iterative scheme in the CG Method to solve <span class="math notranslate nohighlight">\(A\vec x=\vec b\)</span>, where <span class="math notranslate nohighlight">\(A\in M_{n\times n}(\mathbb R)\)</span> is SPD:
$<span class="math notranslate nohighlight">\( 
\vec x_{k+1}=\vec x_{k}+\alpha_k\vec p_k,\qquad k=0,1,2,\cdots
\)</span>$</p>
<p>Suppose that <span class="math notranslate nohighlight">\(\{\vec r_k=A\vec x_k-\vec b\}_{k=0}^{\infty}\)</span> are orthogonal to each other (i.e.<span class="math notranslate nohighlight">\(\vec r_i^T\vec r_j=0\ ,\forall i\neq j\)</span>). Then, the iterative scheme coverges to the solution of <span class="math notranslate nohighlight">\(A\vec x=\vec b\)</span> in <em>less than or equal to</em> <span class="math notranslate nohighlight">\(n\)</span> iterations.</p>
<p><em>Proof by contradiction.</em></p>
<p>Assume that <span class="math notranslate nohighlight">\(\vec r_0,\vec r_1,\cdots,\vec r_n\)</span> are all non-zero. Then <span class="math notranslate nohighlight">\(\{\vec r_0,\vec r_1,\cdots,\vec r_n\}\)</span> forms an orthogonal and linearly independent set in <span class="math notranslate nohighlight">\(\mathbb R^n\)</span>. Since there are <span class="math notranslate nohighlight">\((n+1)\)</span> elements in <span class="math notranslate nohighlight">\(\{\vec r_0,\vec r_1,\cdots,\vec r_n\}\)</span>, it is impossible.</p>
<p>So there must be <span class="math notranslate nohighlight">\(\vec r_i=A\vec x_i-\vec b=\vec 0\)</span> for some <span class="math notranslate nohighlight">\(i\leq n\)</span>. Here this <span class="math notranslate nohighlight">\(\vec x_i\)</span> is the solution of <span class="math notranslate nohighlight">\(A\vec x=\vec b\)</span>. Therefore, the iterative scheme coverges in <span class="math notranslate nohighlight">\(i\leq n\)</span> iterations.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="iterative_methods_1.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Iterative Methods for Solving Linear Systems (1)</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="eigenvalue.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Eigenvalue Problems</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Chenxin Jiang<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>